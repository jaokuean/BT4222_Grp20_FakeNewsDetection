{"cells":[{"cell_type":"markdown","metadata":{"id":"QxRL0-_YRC7E"},"source":["## Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7oLuTmaaJo6"},"outputs":[],"source":["%%capture\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xBAsWzDac4P"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","import torch.utils.data as data_utils\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n","import gc #garbage collector for gpu memory \n","from tqdm import tqdm\n","import json\n","import datetime as dt\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTzZYmRmadlD"},"outputs":[],"source":["from transformers import DistilBertModel, DistilBertTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoConfig, AutoModel\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2500,"status":"ok","timestamp":1649571423672,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"},"user_tz":-480},"id":"T6eVVFsWadwU","outputId":"6faa1361-3f77-414f-c263-b69b4c7f7e69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"yuFqHudayb_3"},"source":["## Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQJpWF7Ytaif"},"outputs":[],"source":["politifact_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/politifact_clean.json\", \"r\"))\n","gossipcop_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/gossipcop_clean.json\", \"r\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKyNLUOatsxT"},"outputs":[],"source":["politifact_df = pd.DataFrame(politifact_data)\n","gossipcop_df = pd.DataFrame(gossipcop_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmi8OF_Rt84h"},"outputs":[],"source":["politifact_df['target'] = politifact_df['label'].apply(lambda x: 1 if x=='real' else 0)\n","gossipcop_df['target'] = gossipcop_df['label'].apply(lambda x: 1 if x=='real' else 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRNBaK9Ra5rJ"},"outputs":[],"source":["politifact_df['parsed_month'] = politifact_df['publish_date'].apply(lambda x: dt.datetime.fromtimestamp(x).strftime(\"%m\") if not pd.isna(x) else '0')\n","gossipcop_df['parsed_month'] = gossipcop_df['publish_date'].apply(lambda x: dt.datetime.fromtimestamp(x).strftime(\"%m\") if not pd.isna(x) else '0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DVuc4tLUtdw"},"outputs":[],"source":["politifact_df['parsed_hour'] = politifact_df['publish_date'].apply(lambda x: dt.datetime.fromtimestamp(x).strftime(\"%H\") if not pd.isna(x) else '0')\n","gossipcop_df['parsed_hour'] = gossipcop_df['publish_date'].apply(lambda x: dt.datetime.fromtimestamp(x).strftime(\"%H\") if not pd.isna(x) else '0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpYIXt-D1HCK"},"outputs":[],"source":["politifact_df['publisher'] = politifact_df['publisher'].fillna('None')\n","gossipcop_df['publisher'] = gossipcop_df['publisher'].fillna('None')"]},{"cell_type":"markdown","metadata":{"id":"fiZs_rf_bJCq"},"source":["## Choose type of article to run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PPbPdRHbJCq"},"outputs":[],"source":["# Choose gossipcop or politifact\n","article = \"gossipcop\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuqZOs8hbJCr"},"outputs":[],"source":["if article == \"politifact\":\n","  X_extra = politifact_df[['parsed_hour','parsed_month', 'publisher']]\n","else:\n","  X_extra = gossipcop_df[['parsed_hour','parsed_month', 'publisher']]"]},{"cell_type":"code","source":["X_extra['parsed_hour'] = X_extra['parsed_hour'].astype('str')\n","X_extra['parsed_month'] = X_extra['parsed_month'].astype('str')\n","X_extra['publisher'] = X_extra['publisher'].astype('str')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZawoC-49mgf","executionInfo":{"status":"ok","timestamp":1649571426535,"user_tz":-480,"elapsed":17,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"}},"outputId":"ddfa65ea-582b-4fc2-a0ce-16ab4c0b6e89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"markdown","metadata":{"id":"CCKT0oL4DhVe"},"source":["## Tokenizing - TF-idf\n","\n","We don't use CV as it performs much poorly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Grq1BMmYtK69"},"outputs":[],"source":["def tokenize(df):\n","    # Get tokenizer\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    # tokenize text\n","    print(\"Title Tokenizing\")\n","    title_tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:98] + ['[SEP]'], tqdm(df['title'])))\n","    # Get token index\n","    title_indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, title_tokenized_df))\n","    \n","    # Pad tokens\n","    totalpadlength = 100\n","    title_index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in title_indexed_tokens])\n","\n","    # Mask\n","    title_mask_variable = [[float(i>0) for i in ii] for ii in title_index_padded]\n","\n","    # Target Variable\n","    target_variable = df['target'].values\n","\n","    # Article text\n","    article_text = df.text_clean.values\n","\n","    return title_index_padded, title_mask_variable, article_text, target_variable\n","\n","def format_tensors(article_text, title_data, title_mask, extra_features, labels, batch_size):\n","    \n","    X_article = torch.from_numpy(article_text)\n","    X_article = X_article.long()\n","\n","    X_title = torch.from_numpy(title_data)\n","    X_title = X_title.long()\n","    title_mask = torch.tensor(title_mask)\n","\n","    extra_features = torch.from_numpy(extra_features)\n","    extra_features = extra_features.long()\n","\n","    y = torch.from_numpy(labels)\n","    y = y.long()\n","\n","    tensordata = data_utils.TensorDataset(X_article, X_title, title_mask, extra_features,  y)\n","    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n","    \n","    return loader\n","\n","def train_validation_test(title_index_padded, title_mask_variable, article_text, X_extra, target_variable, BATCH_SIZE = 8):\n","    # TITLE #\n","    # Train test split for train set\n","    X_train_title, X_rest_title, y_train, y_rest = train_test_split(title_index_padded, target_variable, test_size=0.3, random_state=42)\n","    train_masks_title, rest_masks_title, _, _ = train_test_split(title_mask_variable, title_index_padded, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val_title, X_test_title, y_val, y_test = train_test_split(X_rest_title, y_rest, test_size=0.5, random_state=42)\n","    val_masks_title, test_masks_title, _, _ = train_test_split(rest_masks_title, X_rest_title, test_size=0.5, random_state=42)\n","\n","    # ARTICLE #\n","    # Train test split for train set\n","    X_train_article, X_rest_article, _, _ = train_test_split(article_text, target_variable, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val_article, X_test_article, _, _ = train_test_split(X_rest_article, y_rest, test_size=0.5, random_state=42)\n","\n","    vect = TfidfVectorizer()\n","    X_train_article_dtm = vect.fit_transform(X_train_article).toarray()\n","    X_test_article_dtm = vect.transform(X_test_article).toarray()\n","    X_val_article_dtm = vect.transform(X_val_article).toarray()\n","\n","    # Extra Features #\n","    # Train test split for train set\n","    X_train_extra, X_rest_extra, _, _ = train_test_split(X_extra, target_variable, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val_extra, X_test_extra, _, _ = train_test_split(X_rest_extra, y_rest, test_size=0.5, random_state=42)\n","\n","    enc = OneHotEncoder(handle_unknown = 'ignore')\n","    enc.fit(X_train_extra)\n","    X_train_extra_enc = enc.transform(X_train_extra).toarray()\n","    X_test_extra_enc = enc.transform(X_test_extra).toarray()\n","    X_val_extra_enc = enc.transform(X_val_extra).toarray()\n","\n","\n","    trainloader = format_tensors(X_train_article_dtm, X_train_title, train_masks_title, X_train_extra_enc, y_train, BATCH_SIZE)\n","    validationloader = format_tensors(X_val_article_dtm, X_val_title, val_masks_title, X_val_extra_enc, y_val, BATCH_SIZE)\n","    testloader = format_tensors(X_test_article_dtm, X_test_title, test_masks_title, X_test_extra_enc, y_test, BATCH_SIZE)\n","\n","    return trainloader, validationloader, testloader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQBr8vpMwzlQ","outputId":"c3495016-ad6d-4911-908d-8b162132eeaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Title Tokenizing\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20049/20049 [00:25<00:00, 798.19it/s] \n"]}],"source":["title_index_padded, title_mask_variable, article_text, target_variable = tokenize(politifact_df) if article == \"politifact\" else tokenize(gossipcop_df)\n","trainloader, validationloader, testloader = train_validation_test(title_index_padded, title_mask_variable, article_text, X_extra, target_variable)"]},{"cell_type":"markdown","metadata":{"id":"k6iU1-c36Hq9"},"source":["# Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqHV5JhP6G9a"},"outputs":[],"source":["class BertTfidfMore(torch.nn.Module):\n","    \"\"\"\n","    This takes a transformer backbone and puts a slightly-modified classification head on top.\n","    \n","    \"\"\"\n","\n","    def __init__(self, title_model_name, extra_dims, num_labels):\n","        # num_extra_dims corresponds to the number of extra dimensions of numerical/categorical data\n","\n","        super().__init__()\n","\n","        self.title_transformer = AutoModel.from_pretrained(title_model_name) #title transformer\n","        \n","        for param in self.title_transformer.parameters():\n","          param.requires_grad = False\n","        \n","        num_hidden_size_1 = self.title_transformer.config.hidden_size # May be different depending on which model you use. Common sizes are 768 and 1024. Look in the config.json file \n","        \n","        input_dim = num_hidden_size_1+ extra_dims\n","        \n","        self.fc1 = torch.nn.Linear(input_dim, 1000)\n","        self.fc2 = torch.nn.Linear(1000, 100)\n","        self.classifier = torch.nn.Linear(100, 2)\n","        self.dropout = torch.nn.Dropout(0.25)\n","        self.relu1 = nn.ReLU()\n","        self.relu2 = nn.ReLU()\n","\n","    def forward(self, article_input, title_input_ids, title_attention_mask, extra_features):\n","        \"\"\"\n","        extra_data should be of shape [batch_size, dim] \n","        where dim is the number of additional numerical/categorical dimensions\n","        \"\"\"\n","\n","        title_hidden_states = self.title_transformer(input_ids=title_input_ids, attention_mask=title_attention_mask) # [batch size, sequence length, hidden size]\n","\n","        title_cls_embeds = title_hidden_states.last_hidden_state[:, 0, :] # [batch size, hidden size]\n","\n","        concat = torch.cat((article_input, title_cls_embeds,extra_features), dim=-1) # [batch size, hidden size+num extra dims]\n","        \n","        x = self.fc1(concat)\n","        x = self.dropout(x)\n","        x = self.relu1(x) \n","\n","        x = self.fc2(x)\n","        x = self.dropout(x)\n","        x = self.relu2(x) \n","\n","        output = self.classifier(x) # [batch size, num labels]\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"hY_1fvjZyq4-"},"source":["# Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHsR8djzFyTr"},"outputs":[],"source":["def train_model(epochs, model, learning_rate, start_from_epoch=1):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    torch.cuda.empty_cache() #memory\n","    gc.collect() #memory\n","    NUM_EPOCHS = epochs\n","    loss_function = nn.CrossEntropyLoss()\n","    losses = []\n","    model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    best_acc, best_roc_auc = 0,0\n","    for epoch in range(start_from_epoch, NUM_EPOCHS+1):\n","        model.train()\n","\n","        # For epoch metrics\n","        epoch_loss = 0.0\n","        preds, truth, pred_proba = [],[],[]\n","        iteration = 0\n","        for i, batch in enumerate(tqdm(trainloader)):\n","            iteration += 1\n","            article_input, title_input_ids, title_attention_mask, extra_features, labels = tuple(t.to(device) for t in batch)\n","            optimizer.zero_grad()\n","            outputs = model(article_input, title_input_ids, title_attention_mask, extra_features)\n","            loss = loss_function(outputs, labels)\n","            epoch_loss += float(loss.item())\n","            yhat = outputs\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Metrics for batch\n","            prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","            prediction = (prediction_proba > 0.5).astype(int)\n","            baseline = labels.long().cpu().data.numpy().astype(int)\n","            preds.extend(prediction)\n","            pred_proba.extend(prediction_proba)\n","            truth.extend(baseline)\n","\n","            del article_input, title_input_ids, title_attention_mask, extra_features, labels #memory\n","            torch.cuda.empty_cache() #memory\n","            gc.collect() #memory\n","        \n","\n","        # Calculate train and validation metrics and log them\n","        with torch.set_grad_enabled(False):\n","            metrics = {}\n","            # Training\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Epoch {epoch}:\\nTraining Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Training loss: 'f'{avg_loss}%\\n')\n","            metrics['train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","\n","            # Validation\n","            model.eval()\n","            epoch_loss = 0.0\n","            preds, truth, pred_proba = [],[],[]\n","            iteration = 0\n","            with torch.no_grad():\n","                for i, batch in enumerate(tqdm(validationloader)):\n","                    iteration += 1\n","                    article_input, title_input_ids, title_attention_mask, extra_features, labels = tuple(t.to(device) for t in batch)\n","                    outputs = model(article_input, title_input_ids, title_attention_mask, extra_features)\n","                    \n","                    loss = loss_function(outputs, labels)\n","                    epoch_loss += float(loss.item())\n","                    yhat = outputs\n","\n","                    prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","                    prediction = (prediction_proba > 0.5).astype(int)\n","                    baseline = labels.long().cpu().data.numpy().astype(int)\n","                    preds.extend(prediction)\n","                    pred_proba.extend(prediction_proba)\n","                    truth.extend(baseline)\n","\n","                    del article_input, title_input_ids, title_attention_mask, extra_features, labels #memory\n","                    torch.cuda.empty_cache() #memory\n","                    gc.collect() #memory\n","\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Validation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4255,"status":"ok","timestamp":1649570153785,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"},"user_tz":-480},"id":"ZJwu8jx5w7Ig","colab":{"base_uri":"https://localhost:8080/"},"outputId":"69caca0e-31e4-4f0c-defc-f930701feda4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at gdrive/MyDrive/BT4222/Code/machine_learning/xp/politifact/distilbert_title/model_epoch2 were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["article_model_name = f\"distilbert-base-uncased\"\n","title_model_name = f\"distilbert-base-uncased\"\n","# Path to the best title distilbert model we trained\n","# title_model_name = f\"gdrive/MyDrive/BT4222/Code/machine_learning/xp/{article}/distilbert_title/model_epoch2\" \n","num_labels = 2\n","num_extra_dims = total_dims\n","\n","model = BertTfidfMore(title_model_name, num_extra_dims, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHIy5BcNICSt","executionInfo":{"status":"ok","timestamp":1649570443459,"user_tz":-480,"elapsed":289380,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"}},"outputId":"4bd7de9e-3eeb-46bf-f362-c46d5a35b7ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:24<00:00,  3.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1:\n","Training Accuracy: 0.86%\n","Training ROC AUC: 0.94%\n","Training F1: 0.89%\n","Training loss: 0.4558169176535947%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.87%\n","Validation ROC AUC: 0.94%\n","Validation F1: 0.89%\n","Validation loss: 0.339744944539335%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:23<00:00,  3.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2:\n","Training Accuracy: 0.92%\n","Training ROC AUC: 0.96%\n","Training F1: 0.93%\n","Training loss: 0.24808556781638236%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.89%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.31438237697713906%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3:\n","Training Accuracy: 0.92%\n","Training ROC AUC: 0.97%\n","Training F1: 0.93%\n","Training loss: 0.21583581130419457%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.89%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.30952490907576347%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.98%\n","Training F1: 0.94%\n","Training loss: 0.19250617406907536%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.88%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.3104731132172876%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.98%\n","Training F1: 0.94%\n","Training loss: 0.19088977278165875%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.87%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.89%\n","Validation loss: 0.3112192137373818%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.98%\n","Training F1: 0.94%\n","Training loss: 0.17962796469440773%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.87%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.89%\n","Validation loss: 0.31531665106821394%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.98%\n","Training F1: 0.94%\n","Training loss: 0.1741530832147137%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.88%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.316722866302977%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8:\n","Training Accuracy: 0.92%\n","Training ROC AUC: 0.98%\n","Training F1: 0.93%\n","Training loss: 0.17210416045660773%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.89%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.32389703718945384%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:22<00:00,  3.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.98%\n","Training F1: 0.94%\n","Training loss: 0.16298531502529623%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.88%\n","Validation ROC AUC: 0.95%\n","Validation F1: 0.90%\n","Validation loss: 0.32001280184421277%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 84/84 [00:23<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10:\n","Training Accuracy: 0.93%\n","Training ROC AUC: 0.99%\n","Training F1: 0.94%\n","Training loss: 0.14855309502066424%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:04<00:00,  4.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.89%\n","Validation ROC AUC: 0.94%\n","Validation F1: 0.90%\n","Validation loss: 0.33485094359558487%\n","\n"]}],"source":["train_model(10, model, 3e-5, start_from_epoch=1) "]}],"metadata":{"colab":{"name":"Expt6 Code (TF-IDF of Article Text + Title + Other).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}