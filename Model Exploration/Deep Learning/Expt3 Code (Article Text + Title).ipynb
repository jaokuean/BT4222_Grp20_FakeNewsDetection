{"cells":[{"cell_type":"markdown","metadata":{"id":"QxRL0-_YRC7E"},"source":["## Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7oLuTmaaJo6"},"outputs":[],"source":["%%capture\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xBAsWzDac4P"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n","import torch.utils.data as data_utils\n","import torch.optim as optim\n","import gc #garbage collector for gpu memory \n","from tqdm import tqdm\n","import json\n","import datetime as dt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTzZYmRmadlD"},"outputs":[],"source":["from transformers import DistilBertModel, DistilBertTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoConfig, AutoModel\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39590,"status":"ok","timestamp":1649520581105,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"},"user_tz":-480},"id":"T6eVVFsWadwU","outputId":"5c78cf65-43a1-4d76-e2ee-b07f7202646e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"yuFqHudayb_3"},"source":["## Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQJpWF7Ytaif"},"outputs":[],"source":["politifact_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/politifact_clean.json\", \"r\"))\n","gossipcop_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/gossipcop_clean.json\", \"r\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKyNLUOatsxT"},"outputs":[],"source":["politifact_df = pd.DataFrame(politifact_data)\n","gossipcop_df = pd.DataFrame(gossipcop_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmi8OF_Rt84h"},"outputs":[],"source":["politifact_df['target'] = politifact_df['label'].apply(lambda x: 1 if x=='real' else 0)\n","gossipcop_df['target'] = gossipcop_df['label'].apply(lambda x: 1 if x=='real' else 0)"]},{"cell_type":"markdown","metadata":{"id":"CCKT0oL4DhVe"},"source":["# Tokenizing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Grq1BMmYtK69"},"outputs":[],"source":["def tokenize(df):\n","    # Get tokenizer\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    # tokenize text\n","    print(\"Title Tokenizing\")\n","    title_tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:98] + ['[SEP]'], tqdm(df['title'])))\n","    # Get token index\n","    title_indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, title_tokenized_df))\n","    \n","    # Pad tokens\n","    totalpadlength = 100\n","    title_index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in title_indexed_tokens])\n","\n","    # Mask\n","    title_mask_variable = [[float(i>0) for i in ii] for ii in title_index_padded]\n","\n","\n","    print(\"Article Tokenizing\")\n","    article_tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], tqdm(df['text'])))    \n","    article_indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, article_tokenized_df))\n","  \n","    # Pad tokens\n","    totalpadlength = 512\n","    article_index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in article_indexed_tokens])\n","\n","    # Mask\n","    article_mask_variable = [[float(i>0) for i in ii] for ii in article_index_padded]\n","\n","    # Target Variable\n","    target_variable = df['target'].values\n","\n","    return title_index_padded, title_mask_variable, article_index_padded, article_mask_variable, target_variable\n","\n","def format_tensors(article_data, article_mask, title_data, title_mask, labels, batch_size):\n","    \n","    X_article = torch.from_numpy(article_data)\n","    X_article = X_article.long()\n","    article_mask = torch.tensor(article_mask)\n","\n","    X_title = torch.from_numpy(title_data)\n","    X_title = X_title.long()\n","    title_mask = torch.tensor(title_mask)\n","\n","    y = torch.from_numpy(labels)\n","    y = y.long()\n","\n","    tensordata = data_utils.TensorDataset(X_article, article_mask, X_title, title_mask,  y)\n","    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n","    \n","    return loader\n","\n","def train_validation_test(title_index_padded, title_mask_variable, article_index_padded, article_mask_variable, target_variable, BATCH_SIZE = 8):\n","    # Train test split for train set\n","    X_train_title, X_rest_title, y_train, y_rest = train_test_split(title_index_padded, target_variable, test_size=0.3, random_state=42)\n","    train_masks_title, rest_masks_title, _, _ = train_test_split(title_mask_variable, title_index_padded, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val_title, X_test_title, y_val, y_test = train_test_split(X_rest_title, y_rest, test_size=0.5, random_state=42)\n","    val_masks_title, test_masks_title, _, _ = train_test_split(rest_masks_title, X_rest_title, test_size=0.5, random_state=42)\n","\n","    # Train test split for train set\n","    X_train_article, X_rest_article, y_train, y_rest = train_test_split(article_index_padded, target_variable, test_size=0.3, random_state=42)\n","    train_masks_article, rest_masks_article, _, _ = train_test_split(article_mask_variable, article_index_padded, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val_article, X_test_article, y_val, y_test = train_test_split(X_rest_article, y_rest, test_size=0.5, random_state=42)\n","    val_masks_article, test_masks_article, _, _ = train_test_split(rest_masks_article, X_rest_article, test_size=0.5, random_state=42)\n","\n","    trainloader = format_tensors(X_train_article, train_masks_article, X_train_title, train_masks_title, y_train, BATCH_SIZE)\n","    validationloader = format_tensors(X_val_article, val_masks_article, X_val_title, val_masks_title, y_val, BATCH_SIZE)\n","    testloader = format_tensors(X_test_article, test_masks_article, X_test_title, test_masks_title, y_test, BATCH_SIZE)\n","\n","    return trainloader, validationloader, testloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQBr8vpMwzlQ"},"outputs":[],"source":["# Choose gossipcop or politifact\n","article = \"politifact\"\n","title_index_padded, title_mask_variable, article_index_padded, article_mask_variable, target_variable = tokenize(politifact_df) if article == \"politifact\" else tokenize(gossipcop_df)\n","trainloader, validationloader, testloader = train_validation_test(title_index_padded, title_mask_variable, article_index_padded, article_mask_variable, target_variable)"]},{"cell_type":"markdown","metadata":{"id":"k6iU1-c36Hq9"},"source":["# Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqHV5JhP6G9a"},"outputs":[],"source":["class TwoBert(torch.nn.Module):\n","    \"\"\"\n","    This takes a transformer backbone and puts a slightly-modified classification head on top.\n","    \n","    \"\"\"\n","\n","    def __init__(self, article_model_name, title_model_name, num_labels):\n","        # num_extra_dims corresponds to the number of extra dimensions of numerical/categorical data\n","\n","        super().__init__()\n","\n","        # self.config = AutoConfig.from_pretrained(model_name)\n","        self.article_transformer = AutoModel.from_pretrained(article_model_name) #article transformer\n","        self.title_transformer = AutoModel.from_pretrained(title_model_name) #title transformer\n","\n","        ## Freezing Bert Params so as not to do backprop directly ##\n","        for param in self.article_transformer.parameters():\n","          param.requires_grad = False\n","        \n","        for param in self.title_transformer.parameters():\n","          param.requires_grad = False\n","        \n","        num_hidden_size_1 = self.article_transformer.config.hidden_size\n","        num_hidden_size_2 = self.title_transformer.config.hidden_size # May be different depending on which model you use. Common sizes are 768 and 1024. Look in the config.json file \n","        \n","        input_dim = num_hidden_size_1+num_hidden_size_2\n","        self.linear = torch.nn.Linear(input_dim, 100)\n","        self.classifier = torch.nn.Linear(100, 2)\n","        self.dropout = torch.nn.Dropout(0.25)\n","\n","    def forward(self, article_input_ids, article_attention_mask, title_input_ids, title_attention_mask=None, labels=None):\n","        \"\"\"\n","        extra_data should be of shape [batch_size, dim] \n","        where dim is the number of additional numerical/categorical dimensions\n","        \"\"\"\n","\n","        article_hidden_states = self.article_transformer(input_ids=article_input_ids, attention_mask=article_attention_mask) # [batch size, sequence length, hidden size]\n","\n","        article_cls_embeds = article_hidden_states.last_hidden_state[:, 0, :] # [batch size, hidden size]\n","\n","        title_hidden_states = self.title_transformer(input_ids=title_input_ids, attention_mask=title_attention_mask) # [batch size, sequence length, hidden size]\n","\n","        title_cls_embeds = title_hidden_states.last_hidden_state[:, 0, :] # [batch size, hidden size]\n","\n","        concat = torch.cat((article_cls_embeds, title_cls_embeds), dim=-1) # [batch size, hidden size+num extra dims]\n","        x = self.linear(concat)\n","        x = self.dropout(x) \n","        output = self.classifier(x) # [batch size, num labels]\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"hY_1fvjZyq4-"},"source":["# Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHsR8djzFyTr"},"outputs":[],"source":["def train_model(epochs, model, learning_rate, start_from_epoch=1):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    torch.cuda.empty_cache() #memory\n","    gc.collect() #memory\n","    NUM_EPOCHS = epochs\n","    loss_function = nn.CrossEntropyLoss()\n","    losses = []\n","    model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(start_from_epoch, NUM_EPOCHS+1):\n","        model.train()\n","\n","        # For epoch metrics\n","        epoch_loss = 0.0\n","        preds, truth, pred_proba = [],[],[]\n","        iteration = 0\n","        for i, batch in enumerate(tqdm(trainloader)):\n","            iteration += 1\n","            article_token_ids, article_masks, title_token_ids, title_masks, labels = tuple(t.to(device) for t in batch)\n","            optimizer.zero_grad()\n","            outputs = model(article_token_ids, article_masks, title_token_ids, title_masks, labels)\n","            loss = loss_function(outputs, labels)\n","            epoch_loss += float(loss.item())\n","            yhat = outputs\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Metrics for batch\n","            prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","            prediction = (prediction_proba > 0.5).astype(int)\n","            baseline = labels.long().cpu().data.numpy().astype(int)\n","            preds.extend(prediction)\n","            pred_proba.extend(prediction_proba)\n","            truth.extend(baseline)\n","\n","            del article_token_ids, article_masks, title_token_ids, title_masks, labels #memory\n","            torch.cuda.empty_cache() #memory\n","            gc.collect() #memory\n","        \n","\n","        # Calculate train and validation metrics and log them\n","        with torch.set_grad_enabled(False):\n","            metrics = {}\n","            # Training\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Epoch {epoch}:\\nTraining Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Training loss: 'f'{avg_loss}%\\n')\n","            metrics['train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","\n","            # Validation\n","            model.eval()\n","            epoch_loss = 0.0\n","            preds, truth, pred_proba = [],[],[]\n","            iteration = 0\n","            with torch.no_grad():\n","                for i, batch in enumerate(tqdm(validationloader)):\n","                    iteration += 1\n","                    article_token_ids, article_masks, title_token_ids, title_masks, labels = tuple(t.to(device) for t in batch)\n","                    outputs = model(article_token_ids, article_masks, title_token_ids, title_masks, labels)\n","                    loss = loss_function(outputs, labels)\n","                    yhat = outputs\n","\n","                    # Metrics for batch\n","                    epoch_loss += float(loss.item())\n","                    prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","                    prediction = (prediction_proba > 0.5).astype(int)\n","                    baseline = labels.long().cpu().data.numpy().astype(int)\n","                    preds.extend(prediction)\n","                    pred_proba.extend(prediction_proba)\n","                    truth.extend(baseline)\n","\n","                    del article_token_ids, article_masks, title_token_ids, title_masks, labels #memory\n","                    torch.cuda.empty_cache() #memory\n","                    gc.collect() #memory\n","\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Validation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4851,"status":"ok","timestamp":1649527515611,"user":{"displayName":"Neil Shah","userId":"05348588193969355677"},"user_tz":-480},"id":"ZJwu8jx5w7Ig","outputId":"e50da490-f3f5-4a3e-8648-bc3fbc01e568"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at gdrive/MyDrive/BT4222/Code/machine_learning/xp/politifact/distilbert_fulltext/model_epoch4 were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at gdrive/MyDrive/BT4222/Code/machine_learning/xp/politifact/distilbert_title/model_epoch2 were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["article_model_name = f\"gdrive/MyDrive/BT4222/Code/machine_learning/xp/{article}/distilbert_fulltext/model_epoch4\"\n","title_model_name = f\"gdrive/MyDrive/BT4222/Code/machine_learning/xp/{article}/distilbert_title/model_epoch2\"\n","num_labels = 2\n","\n","model = TwoBert(article_model_name, title_model_name, num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"oHIy5BcNICSt","outputId":"e43eab75-ec6f-4a75-ad17-3ff47d83525a"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:03<00:00,  9.33s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1:\n","Training Accuracy: 0.91%\n","Training ROC AUC: 0.97%\n","Training F1: 0.92%\n","Training loss: 0.2726523131575613%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:33<00:00,  8.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.92%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.93%\n","Validation loss: 0.1860288812054528%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:19<00:00,  9.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.96%\n","Training loss: 0.1503038842984963%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:34<00:00,  8.59s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.1591207477160626%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:27<00:00,  9.62s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3:\n","Training Accuracy: 0.95%\n","Training ROC AUC: 0.99%\n","Training F1: 0.96%\n","Training loss: 0.13115712581202388%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:35<00:00,  8.64s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.93%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.14968557433328694%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:28<00:00,  9.63s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4:\n","Training Accuracy: 0.95%\n","Training ROC AUC: 0.99%\n","Training F1: 0.96%\n","Training loss: 0.1265318715346179%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:38<00:00,  8.81s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.93%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.14521858520391914%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:31<00:00,  9.66s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.1172439312019075%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:35<00:00,  8.66s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.93%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.14137776454703677%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:29<00:00,  9.63s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.11468564162385605%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:35<00:00,  8.64s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.13902583389749956%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:25<00:00,  9.59s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.11019266198854893%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:35<00:00,  8.66s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.94%\n","Validation loss: 0.13694108973464203%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:37<00:00,  9.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.11118656589782663%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:37<00:00,  8.76s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.95%\n","Validation loss: 0.13498282147985366%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:35<00:00,  9.71s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.10967704583890736%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:42<00:00,  9.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.95%\n","Validation loss: 0.13276954160796273%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 84/84 [13:32<00:00,  9.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10:\n","Training Accuracy: 0.97%\n","Training ROC AUC: 0.99%\n","Training F1: 0.97%\n","Training loss: 0.10174257711263462%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 18/18 [02:36<00:00,  8.71s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.94%\n","Validation ROC AUC: 0.99%\n","Validation F1: 0.95%\n","Validation loss: 0.13070973632339802%\n","\n"]}],"source":["train_model(10, model, 3e-5, start_from_epoch=1)"]}],"metadata":{"colab":{"name":"Expt3 Code (Article Text + Title).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}