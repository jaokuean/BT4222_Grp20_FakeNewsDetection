{"cells":[{"cell_type":"markdown","metadata":{"id":"DIqSUU-ZyQOX"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXZ6vXTxtP1r"},"outputs":[],"source":["%%capture\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23964,"status":"ok","timestamp":1649440566550,"user":{"displayName":"xingpeng wang","userId":"18144496759074956184"},"user_tz":-480},"id":"byIdshb9-Uky","outputId":"d055ef8d-17ec-4d45-fcaf-90c44b18d135"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n","import torch.utils.data as data_utils\n","import torch.optim as optim\n","import gc #garbage collector for gpu memory \n","from tqdm import tqdm\n","import json\n","\n","from transformers import BertForSequenceClassification, BertTokenizer, DistilBertModel, DistilBertTokenizer\n","from transformers import AutoModelForSequenceClassification\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"yuFqHudayb_3"},"source":["# Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQJpWF7Ytaif"},"outputs":[],"source":["politifact_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/politifact_clean.json\", \"r\"))\n","gossipcop_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/gossipcop_clean.json\", \"r\"))\n","\n","# Convert list of json objects to dataframe\n","politifact_df = pd.DataFrame(politifact_data)\n","gossipcop_df = pd.DataFrame(gossipcop_data)\n","\n","# Conver labels to integers\n","politifact_df['target'] = politifact_df['label'].apply(lambda x: 1 if x=='real' else 0)\n","gossipcop_df['target'] = gossipcop_df['label'].apply(lambda x: 1 if x=='real' else 0)"]},{"cell_type":"markdown","metadata":{"id":"CCKT0oL4DhVe"},"source":["# Tokenizing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Grq1BMmYtK69"},"outputs":[],"source":["def tokenize(df):\n","    # Get tokenizer\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    # tokenize text\n","    print(\"Tokenizing\")\n","    tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:100] + ['[SEP]'], tqdm(df['title'])))\n","    # Get token index\n","    indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_df))\n","    # Pad tokens\n","    totalpadlength = 100\n","    index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in indexed_tokens])\n","    target_variable = df['target'].values\n","\n","    # Mask\n","    mask_variable = [[float(i>0) for i in ii] for ii in index_padded]\n","\n","    return index_padded, mask_variable, target_variable\n","\n","def format_tensors(text_data, mask, labels, batch_size):\n","    X = torch.from_numpy(text_data)\n","    X = X.long()\n","    mask = torch.tensor(mask)\n","    y = torch.from_numpy(labels)\n","    y = y.long()\n","    tensordata = data_utils.TensorDataset(X, mask, y)\n","    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n","    return loader\n","\n","def train_validation_test(index_padded, mask_variable, target_variable, BATCH_SIZE = 8):\n","    # Train test split for train set\n","    X_train, X_rest, y_train, y_rest = train_test_split(index_padded, target_variable, test_size=0.3, random_state=42)\n","    train_masks, rest_masks, _, _ = train_test_split(mask_variable, index_padded, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)\n","    val_masks, test_masks, _, _ = train_test_split(rest_masks, X_rest, test_size=0.5, random_state=42)\n","\n","    trainloader = format_tensors(X_train, train_masks, y_train, BATCH_SIZE)\n","    validationloader = format_tensors(X_val, val_masks, y_val, BATCH_SIZE)\n","    testloader = format_tensors(X_test, test_masks, y_test, BATCH_SIZE)\n","\n","    return trainloader, validationloader, testloader"]},{"cell_type":"markdown","metadata":{"id":"IZqqCwC_HmtN"},"source":["# Choose type of article to run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15366,"status":"ok","timestamp":1649441754575,"user":{"displayName":"xingpeng wang","userId":"18144496759074956184"},"user_tz":-480},"id":"WQBr8vpMwzlQ","outputId":"2e8f34eb-4cfc-4d9e-f03e-b7360b2805aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 20049/20049 [00:12<00:00, 1640.36it/s]\n"]}],"source":["# Choose gossipcop or politifact\n","article = \"gossipcop\"\n","index_padded, mask_variable, target_variable = tokenize(politifact_df) if article == \"politifact\" else tokenize(gossipcop_df)\n","trainloader, validationloader, testloader = train_validation_test(index_padded, mask_variable, target_variable)"]},{"cell_type":"markdown","metadata":{"id":"hY_1fvjZyq4-"},"source":["# Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g48Vy2IyxIOB"},"outputs":[],"source":["def validation_metrics(model, device, dataloader):\n","    tqdm()\n","    model.eval()\n","    preds, truth, pred_proba = [],[],[]\n","    with torch.no_grad():\n","        for i, batch in enumerate(tqdm(dataloader)):\n","            token_ids, masks, labels = tuple(t.to(device) for t in batch)\n","            output = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n","            loss = output['loss']\n","            yhat = output['logits']\n","            predicition_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","            prediction = (predicition_proba > 0.5).astype(int)\n","            baseline = labels.long().cpu().data.numpy().astype(int)\n","            preds.extend(prediction)\n","            pred_proba.extend(predicition_proba)\n","            truth.extend(baseline)\n","            del token_ids, masks, labels #memory        \n","        torch.cuda.empty_cache() #memory\n","        gc.collect() # memory\n","        return accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHsR8djzFyTr"},"outputs":[],"source":["def train_model(epochs, model, learning_rate, start_from_epoch=1):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    torch.cuda.empty_cache() #memory\n","    gc.collect() #memory\n","    NUM_EPOCHS = epochs\n","    loss_function = nn.BCEWithLogitsLoss()\n","    losses = []\n","    model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    for epoch in range(start_from_epoch, NUM_EPOCHS+1):\n","        model.train()\n","\n","        # For epoch metrics\n","        epoch_loss = 0.0\n","        preds, truth, pred_proba = [],[],[]\n","        iteration = 0\n","        for i, batch in enumerate(tqdm(trainloader)):\n","            iteration += 1\n","            token_ids, masks, labels = tuple(t.to(device) for t in batch)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n","            loss = outputs['loss']\n","            epoch_loss += float(loss.item())\n","            yhat = outputs['logits']\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Metrics for batch\n","            prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","            prediction = (prediction_proba > 0.5).astype(int)\n","            baseline = labels.long().cpu().data.numpy().astype(int)\n","            preds.extend(prediction)\n","            pred_proba.extend(prediction_proba)\n","            truth.extend(baseline)\n","\n","            del token_ids, masks, labels #memory\n","            torch.cuda.empty_cache() #memory\n","            gc.collect() #memory\n","        \n","\n","        # Calculate train and validation metrics and log them\n","        with torch.set_grad_enabled(False):\n","            metrics = {}\n","            # Training\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Epoch {epoch}:\\nTraining Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Training loss: 'f'{avg_loss}%\\n')\n","            metrics['train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","\n","            # Validation\n","            model.eval()\n","            epoch_loss = 0.0\n","            preds, truth, pred_proba = [],[],[]\n","            iteration = 0\n","            with torch.no_grad():\n","                for i, batch in enumerate(tqdm(validationloader)):\n","                    iteration += 1\n","                    token_ids, masks, labels = tuple(t.to(device) for t in batch)\n","                    outputs = model(input_ids=token_ids, attention_mask=masks, labels=labels)\n","                    loss = outputs['loss']\n","                    yhat = outputs['logits']\n","\n","                    # Metrics for batch\n","                    epoch_loss += float(loss.item())\n","                    prediction_proba = torch.sigmoid(yhat[:,1]).cpu().data.numpy()\n","                    prediction = (prediction_proba > 0.5).astype(int)\n","                    baseline = labels.long().cpu().data.numpy().astype(int)\n","                    preds.extend(prediction)\n","                    pred_proba.extend(prediction_proba)\n","                    truth.extend(baseline)\n","\n","                    del token_ids, masks, labels #memory\n","                    torch.cuda.empty_cache() #memory\n","                    gc.collect() #memory\n","\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), epoch_loss/float(iteration)\n","            print(f'Validation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1263,"status":"ok","timestamp":1649441755822,"user":{"displayName":"xingpeng wang","userId":"18144496759074956184"},"user_tz":-480},"id":"ZJwu8jx5w7Ig","outputId":"f00b3b43-2049-4183-f5d9-f1355d052db8"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"oHIy5BcNICSt","outputId":"cad16d67-3c2a-4cee-90a1-cb6def8cbab2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:39<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1:\n","Training Accuracy: 0.82%\n","Training ROC AUC: 0.83%\n","Training F1: 0.89%\n","Training loss: 0.406923114952029%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:48<00:00,  3.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.85%\n","Validation ROC AUC: 0.88%\n","Validation F1: 0.90%\n","Validation loss: 0.3495739454879089%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:51<00:00,  2.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2:\n","Training Accuracy: 0.88%\n","Training ROC AUC: 0.92%\n","Training F1: 0.92%\n","Training loss: 0.2937586132681098%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:52<00:00,  3.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.85%\n","Validation ROC AUC: 0.89%\n","Validation F1: 0.90%\n","Validation loss: 0.35457066210244403%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:52<00:00,  2.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3:\n","Training Accuracy: 0.92%\n","Training ROC AUC: 0.96%\n","Training F1: 0.95%\n","Training loss: 0.20758854888347733%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:45<00:00,  3.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.83%\n","Validation ROC AUC: 0.88%\n","Validation F1: 0.89%\n","Validation loss: 0.42230669840516405%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:24<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4:\n","Training Accuracy: 0.95%\n","Training ROC AUC: 0.98%\n","Training F1: 0.97%\n","Training loss: 0.13685281862096166%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:46<00:00,  3.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.83%\n","Validation ROC AUC: 0.87%\n","Validation F1: 0.88%\n","Validation loss: 0.5059392544604402%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:21<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5:\n","Training Accuracy: 0.96%\n","Training ROC AUC: 0.99%\n","Training F1: 0.98%\n","Training loss: 0.09991935837444746%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:45<00:00,  3.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.85%\n","Validation ROC AUC: 0.87%\n","Validation F1: 0.90%\n","Validation loss: 0.5723193709508199%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:21<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6:\n","Training Accuracy: 0.97%\n","Training ROC AUC: 0.99%\n","Training F1: 0.98%\n","Training loss: 0.0806501167557372%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:44<00:00,  3.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.85%\n","Validation ROC AUC: 0.87%\n","Validation F1: 0.90%\n","Validation loss: 0.612450985575901%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:22<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7:\n","Training Accuracy: 0.97%\n","Training ROC AUC: 1.00%\n","Training F1: 0.98%\n","Training loss: 0.06772559544032086%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:45<00:00,  3.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.83%\n","Validation ROC AUC: 0.86%\n","Validation F1: 0.89%\n","Validation loss: 0.6247016264112082%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:26<00:00,  2.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8:\n","Training Accuracy: 0.98%\n","Training ROC AUC: 1.00%\n","Training F1: 0.98%\n","Training loss: 0.061120408397468746%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:45<00:00,  3.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.83%\n","Validation ROC AUC: 0.85%\n","Validation F1: 0.89%\n","Validation loss: 0.6670782530693922%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:20<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9:\n","Training Accuracy: 0.98%\n","Training ROC AUC: 1.00%\n","Training F1: 0.99%\n","Training loss: 0.05244513199957515%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:45<00:00,  3.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.84%\n","Validation ROC AUC: 0.86%\n","Validation F1: 0.90%\n","Validation loss: 0.6684694772199518%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1755/1755 [12:23<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10:\n","Training Accuracy: 0.98%\n","Training ROC AUC: 1.00%\n","Training F1: 0.99%\n","Training loss: 0.04929513858344029%\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 376/376 [01:44<00:00,  3.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.84%\n","Validation ROC AUC: 0.86%\n","Validation F1: 0.90%\n","Validation loss: 0.6812157819931212%\n","\n"]}],"source":["train_model(10, model, 1e-5, start_from_epoch=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvifqVHwY6cv"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"title_train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}