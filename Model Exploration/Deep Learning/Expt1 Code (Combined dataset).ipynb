{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Expt1 Code (Combined dataset).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"DIqSUU-ZyQOX"}},{"cell_type":"code","source":["%%capture\n","!pip install transformers"],"metadata":{"id":"UXZ6vXTxtP1r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byIdshb9-Uky","executionInfo":{"status":"ok","timestamp":1649868241169,"user_tz":-480,"elapsed":29045,"user":{"displayName":"Wang Xing Peng","userId":"13203375634952968736"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"be79af57-ab5f-4e01-9cff-0120ba7dfaa3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n","import torch.utils.data as data_utils\n","import torch.optim as optim\n","import gc #garbage collector for gpu memory \n","from tqdm import tqdm\n","import json\n","\n","from transformers import BertForSequenceClassification, BertTokenizer, DistilBertModel, DistilBertTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoConfig, AutoModel\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["# Loading Data"],"metadata":{"id":"yuFqHudayb_3"}},{"cell_type":"code","source":["politifact_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/politifact_clean.json\", \"r\"))\n","gossipcop_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/gossipcop_clean.json\", \"r\"))\n","\n","# Convert list of json objects to dataframe\n","politifact_df = pd.DataFrame(politifact_data)\n","gossipcop_df = pd.DataFrame(gossipcop_data)\n","\n","# Conver labels to integers\n","politifact_df['target'] = politifact_df['label'].apply(lambda x: 1 if x=='real' else 0)\n","gossipcop_df['target'] = gossipcop_df['label'].apply(lambda x: 1 if x=='real' else 0)\n","\n","# Label to differentiate politifact and gossipcop\n","politifact_df['is_pf'] = 1\n","gossipcop_df['is_pf'] = 0\n","\n","# Concatenate the two datasets\n","concat_df = pd.concat([politifact_df,gossipcop_df])"],"metadata":{"id":"fQJpWF7Ytaif"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing"],"metadata":{"id":"CCKT0oL4DhVe"}},{"cell_type":"code","source":["def tokenize(df, index_padded=None):\n","    # Get tokenizer\n","    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    # tokenize text\n","    print(\"Tokenizing\")\n","    tokenized_df = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], tqdm(df['text'])))\n","    # Get token index\n","    indexed_tokens = list(map(tokenizer.convert_tokens_to_ids, tokenized_df))\n","    # Pad tokens\n","    totalpadlength = 512\n","    index_padded = np.array([xi+[0]*(totalpadlength-len(xi)) for xi in indexed_tokens])\n","        \n","    target_variable = df['target'].values\n","    article_flag = df[['is_pf']]\n","\n","    # Mask\n","    mask_variable = [[float(i>0) for i in ii] for ii in index_padded]\n","\n","    return index_padded, mask_variable, target_variable, article_flag\n","\n","def format_tensors(text_data, mask, labels, batch_size, flag):\n","    X = torch.from_numpy(text_data)\n","    X = X.long()\n","    mask = torch.tensor(mask)\n","    y = torch.from_numpy(labels)\n","    y = y.long()\n","    numerical_data = torch.from_numpy(flag)\n","    numerical_data = numerical_data.long()\n","\n","    tensordata = data_utils.TensorDataset(X, mask, numerical_data, y)\n","    loader = data_utils.DataLoader(tensordata, batch_size=batch_size, shuffle=False)\n","    return loader\n","\n","def train_validation_test(index_padded, mask_variable, target_variable, article_flag, BATCH_SIZE = 8):\n","    # Train test split for train set\n","    X_train, X_rest, y_train, y_rest = train_test_split(index_padded, target_variable, test_size=0.3, random_state=42)\n","    train_masks, rest_masks, _, _ = train_test_split(mask_variable, index_padded, test_size=0.3, random_state=42)\n","    X_flag_train, X_flag_rest, _, _ = train_test_split(article_flag, index_padded, test_size=0.3, random_state=42)\n","\n","    # Train test split again for validation and test set\n","    X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)\n","    val_masks, test_masks, _, _ = train_test_split(rest_masks, X_rest, test_size=0.5, random_state=42)\n","    X_flag_val, X_flag_test, _, _ = train_test_split(X_flag_rest, X_rest, test_size=0.5, random_state=42)\n","\n","    X_flag_train  = X_flag_train.is_pf.to_numpy()\n","    X_flag_val  = X_flag_val.is_pf.to_numpy()\n","    X_flag_test  = X_flag_test.is_pf.to_numpy()\n","\n","    trainloader = format_tensors(X_train, train_masks, y_train, BATCH_SIZE, X_flag_train.reshape(-1))\n","    validationloader = format_tensors(X_val, val_masks, y_val, BATCH_SIZE, X_flag_val.reshape(-1))\n","    testloader = format_tensors(X_test, test_masks, y_test, BATCH_SIZE, X_flag_test.reshape(-1))\n","\n","    return trainloader, validationloader, testloader"],"metadata":{"id":"Grq1BMmYtK69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_path = \"gdrive/MyDrive/BT4222/Code/machine_learning/xp/combined/\"\n","# Tokenize data and obtain necessary input features\n","index_padded, mask_variable, target_variable, article_flag = tokenize(concat_df)\n","# Create train validation test loaders\n","trainloader, validationloader, testloader = train_validation_test(index_padded, mask_variable, target_variable, article_flag)"],"metadata":{"id":"WQBr8vpMwzlQ","executionInfo":{"status":"ok","timestamp":1649398361809,"user_tz":-480,"elapsed":10499,"user":{"displayName":"Wang Xing Peng","userId":"13203375634952968736"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"38c636e1-f46a-4d11-9aed-65e28bd96c36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing\n"]}]},{"cell_type":"markdown","source":["# Model Creation"],"metadata":{"id":"hY_1fvjZyq4-"}},{"cell_type":"code","source":["class BertAndFlag(torch.nn.Module):\n","    \"\"\"\n","    This takes a transformer backbone and puts a slightly-modified classification head on top.\n","    \n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.transformer = AutoModel.from_pretrained('distilbert-base-uncased') #Article transformer\n","        num_hidden_size = self.transformer.config.hidden_size # May be different depending on which model you use. Common sizes are 768 and 1024. Look in the config.json file \n","        self.classifier = torch.nn.Linear(num_hidden_size+1, 2)\n","\n","    def forward(self, input_ids, extra_data, attention_mask=None, labels=None):\n","        \"\"\"\n","        extra_data should be of shape [batch_size, dim] \n","        where dim is the number of additional numerical/categorical dimensions\n","        \"\"\"\n","        hidden_states = self.transformer(input_ids=input_ids, attention_mask=attention_mask) # [batch size, sequence length, hidden size]\n","        cls_embeds = hidden_states.last_hidden_state[:, 0, :] # [batch size, hidden size]\n","        concat = torch.cat((cls_embeds, extra_data.unsqueeze(dim=-1)), dim=-1) # [batch size, hidden size+num extra dims]\n","        output = self.classifier(concat)\n","        return output"],"metadata":{"id":"8x35gS3zKr4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def weighted_loss(loss_function, outputs, labels, is_pf, weights):\n","    '''\n","    loss_function outputs a 1D Tensor of Shape [1,]\n","    outputs: model outputs of shape [batch_size, 2 ]\n","    is_df : 1D tensor of shape [batch_size]\n","    weights: 1d tensor of shape [2] where first value corresponds to weight\n","    for pf and second for gc\n","    '''\n","    loss = loss_function(outputs,labels)\n","    #print(loss)\n","\n","    weight_vec = is_pf*weights[0] + (1-is_pf)*weights[1]\n","    #print(weight_vec)\n","    weighted_loss = weight_vec.mean()*loss\n","    #print(weighted_loss)\n","    return weighted_loss"],"metadata":{"id":"ftwvTdhv_Ksf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(epochs, model, learning_rate, start_from_epoch=1):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    torch.cuda.empty_cache() #memory\n","    gc.collect() #memory\n","    NUM_EPOCHS = epochs\n","    loss_function = nn.CrossEntropyLoss()\n","    model.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    # Loss weight\n","    weight = [0,0]\n","    weight[0] = concat_df.shape[0]/(2*politifact_df.shape[0])\n","    weight[1] = concat_df.shape[0]/(2*gossipcop_df.shape[0])\n","    weight = torch.tensor(weight).to(device)\n","\n","    for epoch in range(start_from_epoch, NUM_EPOCHS+1):\n","        model.train()\n","\n","        # For epoch metrics\n","        epoch_loss = []\n","        preds, truth, pred_proba, flag = [],[],[],[]\n","        iteration = 0\n","        for i, batch in enumerate(tqdm(trainloader)):\n","            iteration += 1\n","            token_ids, masks, numerical, labels = tuple(t.to(device) for t in batch)\n","            optimizer.zero_grad()\n","            outputs = model(input_ids=token_ids, extra_data=numerical, attention_mask=masks, labels=labels)\n","            loss = weighted_loss(loss_function, outputs, labels, numerical, weight)\n","            epoch_loss.append(float(loss.item()))\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Metrics for batch\n","            prediction_proba = torch.sigmoid(outputs[:,1]).cpu().data.numpy()\n","            prediction = (prediction_proba > 0.5).astype(int)\n","            baseline = labels.long().cpu().data.numpy().astype(int)\n","            article_flag = numerical.long().cpu().data.numpy().astype(int)\n","            preds.extend(prediction)\n","            pred_proba.extend(prediction_proba)\n","            truth.extend(baseline)\n","            flag.extend(article_flag)\n","\n","            del token_ids, masks, numerical, labels #memory\n","            torch.cuda.empty_cache() #memory\n","            gc.collect() #memory\n","        \n","\n","        # Calculate train and validation metrics and log them\n","        with torch.set_grad_enabled(False):\n","            metrics = {}\n","            # Training\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), sum(epoch_loss)/float(iteration)\n","            print(f'Epoch {epoch}:\\nTraining Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Training loss: 'f'{avg_loss}%\\n')\n","            metrics['train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","            epoch_results = list(zip(preds, truth, pred_proba, epoch_loss, flag))\n","\n","            # Politifact Training\n","            politifact_results = list(filter(lambda x: x[4]==1, epoch_results))\n","            politifact_preds = list(map(lambda x: x[0], politifact_results))\n","            politifact_truth = list(map(lambda x: x[1], politifact_results))\n","            politifact_pred_proba = list(map(lambda x: x[2], politifact_results))\n","            politifact_loss = list(map(lambda x: x[3], politifact_results))\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(politifact_truth, politifact_preds), roc_auc_score(politifact_truth, politifact_pred_proba), f1_score(politifact_truth, politifact_preds), sum(politifact_loss)/len(politifact_loss)\n","            print(f'Politifact Training Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Politifact Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Politifact Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Politifact Training loss: 'f'{avg_loss}%\\n')\n","            metrics['politifact_train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","            # Gossipcop Training\n","            gossipcop_results = list(filter(lambda x: x[4]==0, epoch_results))\n","            gossipcop_preds = list(map(lambda x: x[0], gossipcop_results))\n","            gossipcop_truth = list(map(lambda x: x[1], gossipcop_results))\n","            gossipcop_pred_proba = list(map(lambda x: x[2], gossipcop_results))\n","            gossipcop_loss = list(map(lambda x: x[3], gossipcop_results))\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(gossipcop_truth, gossipcop_preds), roc_auc_score(gossipcop_truth, gossipcop_pred_proba), f1_score(gossipcop_truth, gossipcop_preds), sum(gossipcop_loss)/len(gossipcop_loss)\n","            print(f'Gossipcop Training Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Gossipcop Training ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Gossipcop Training F1: 'f'{avg_f1:.2f}%')\n","            print(f'Gossipcop Training loss: 'f'{avg_loss}%\\n')\n","            metrics['gossipcop_train'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","\n","            # Validation\n","            model.eval()\n","            epoch_loss = []\n","            preds, truth, pred_proba, flag = [],[],[],[]\n","            iteration = 0\n","            with torch.no_grad():\n","                for i, batch in enumerate(tqdm(validationloader)):\n","                    iteration += 1\n","                    token_ids, masks, numerical, labels = tuple(t.to(device) for t in batch)\n","                    outputs = model(input_ids=token_ids, extra_data=numerical, attention_mask=masks, labels=labels)\n","                    loss = loss_function(outputs, labels)\n","\n","                    # Metrics for batch\n","                    epoch_loss.append(float(loss.item()))\n","                    prediction_proba = torch.sigmoid(outputs[:,1]).cpu().data.numpy()\n","                    prediction = (prediction_proba > 0.5).astype(int)\n","                    baseline = labels.long().cpu().data.numpy().astype(int)\n","                    article_flag = numerical.long().cpu().data.numpy().astype(int)\n","                    preds.extend(prediction)\n","                    pred_proba.extend(prediction_proba)\n","                    truth.extend(baseline)\n","                    flag.extend(article_flag)\n","\n","                    del token_ids, masks, numerical, labels #memory\n","                    torch.cuda.empty_cache() #memory\n","                    gc.collect() #memory\n","\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(truth, preds), roc_auc_score(truth, pred_proba), f1_score(truth, preds), sum(epoch_loss)/float(iteration)\n","            print(f'Epoch {epoch}:\\nValidation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","            epoch_results = list(zip(preds, truth, pred_proba, epoch_loss, flag))\n","\n","            # Politifact Validation\n","            politifact_results = list(filter(lambda x: x[4]==1, epoch_results))\n","            politifact_preds = list(map(lambda x: x[0], politifact_results))\n","            politifact_truth = list(map(lambda x: x[1], politifact_results))\n","            politifact_pred_proba = list(map(lambda x: x[2], politifact_results))\n","            politifact_loss = list(map(lambda x: x[3], politifact_results))\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(politifact_truth, politifact_preds), roc_auc_score(politifact_truth, politifact_pred_proba), f1_score(politifact_truth, politifact_preds), sum(politifact_loss)/len(politifact_loss)\n","            print(f'Politifact Validation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Politifact Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Politifact Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Politifact Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['politifact_validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n","            # Gossipcop Validation\n","            gossipcop_results = list(filter(lambda x: x[4]==0, epoch_results))\n","            gossipcop_preds = list(map(lambda x: x[0], gossipcop_results))\n","            gossipcop_truth = list(map(lambda x: x[1], gossipcop_results))\n","            gossipcop_pred_proba = list(map(lambda x: x[2], gossipcop_results))\n","            gossipcop_loss = list(map(lambda x: x[3], gossipcop_results))\n","            avg_accuracy, avg_roc_auc, avg_f1, avg_loss = accuracy_score(gossipcop_truth, gossipcop_preds), roc_auc_score(gossipcop_truth, gossipcop_pred_proba), f1_score(gossipcop_truth, gossipcop_preds), sum(gossipcop_loss)/len(gossipcop_loss)\n","            print(f'Gossipcop Validation Accuracy: 'f'{avg_accuracy:.2f}%')\n","            print(f'Gossipcop Validation ROC AUC: 'f'{avg_roc_auc:.2f}%')\n","            print(f'Gossipcop Validation F1: 'f'{avg_f1:.2f}%')\n","            print(f'Gossipcop Validation loss: 'f'{avg_loss}%\\n')\n","            metrics['gossipcop_validation'] = {\n","                'accuracy':avg_accuracy,\n","                'roc_auc':avg_roc_auc,\n","                'f1':avg_f1,\n","                'loss':avg_loss\n","            }\n"],"metadata":{"id":"iHsR8djzFyTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BertAndFlag(torch.nn.Module)"],"metadata":{"id":"Y3fYdF0R3FkZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(10, model, 1e-5, start_from_epoch=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oHIy5BcNICSt","outputId":"2a69fb14-eeb1-4ca0-be1c-27b5756b1159","executionInfo":{"status":"error","timestamp":1649407395558,"user_tz":-480,"elapsed":7956112,"user":{"displayName":"Wang Xing Peng","userId":"13203375634952968736"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1838/1838 [38:19<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4:\n","Training Accuracy: 0.92%\n","Training ROC AUC: 0.96%\n","Training F1: 0.95%\n","Training loss: 0.15875153909990933%\n","\n","Politifact Training Accuracy: 0.86%\n","Politifact Training ROC AUC: 0.98%\n","Politifact Training F1: 0.88%\n","Politifact Training loss: 0.1529859317190669%\n","\n","Gossipcop Training Accuracy: 0.92%\n","Gossipcop Training ROC AUC: 0.95%\n","Gossipcop Training F1: 0.95%\n","Gossipcop Training loss: 0.15901734114763172%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 394/394 [03:19<00:00,  1.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4:\n","Validation Accuracy: 0.83%\n","Validation ROC AUC: 0.84%\n","Validation F1: 0.89%\n","Validation loss: 0.5000742741855722%\n","\n","Politifact Validation Accuracy: 0.79%\n","Politifact Validation ROC AUC: 0.82%\n","Politifact Validation F1: 0.86%\n","Politifact Validation loss: 0.3651072084903717%\n","\n","Gossipcop Validation Accuracy: 0.86%\n","Gossipcop Validation ROC AUC: 0.86%\n","Gossipcop Validation F1: 0.91%\n","Gossipcop Validation loss: 0.5050467450269743%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1838/1838 [38:18<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5:\n","Training Accuracy: 0.94%\n","Training ROC AUC: 0.97%\n","Training F1: 0.96%\n","Training loss: 0.12275895621724223%\n","\n","Politifact Training Accuracy: 0.91%\n","Politifact Training ROC AUC: 0.98%\n","Politifact Training F1: 0.92%\n","Politifact Training loss: 0.137535247106657%\n","\n","Gossipcop Training Accuracy: 0.94%\n","Gossipcop Training ROC AUC: 0.97%\n","Gossipcop Training F1: 0.96%\n","Gossipcop Training loss: 0.12207774986434378%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 394/394 [03:20<00:00,  1.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5:\n","Validation Accuracy: 0.84%\n","Validation ROC AUC: 0.85%\n","Validation F1: 0.90%\n","Validation loss: 0.5581347529875672%\n","\n","Politifact Validation Accuracy: 0.71%\n","Politifact Validation ROC AUC: 0.78%\n","Politifact Validation F1: 0.82%\n","Politifact Validation loss: 0.37202364176378716%\n","\n","Gossipcop Validation Accuracy: 0.86%\n","Gossipcop Validation ROC AUC: 0.86%\n","Gossipcop Validation F1: 0.91%\n","Gossipcop Validation loss: 0.5649914781379171%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1838/1838 [38:19<00:00,  1.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6:\n","Training Accuracy: 0.95%\n","Training ROC AUC: 0.98%\n","Training F1: 0.97%\n","Training loss: 0.10548447246436965%\n","\n","Politifact Training Accuracy: 0.93%\n","Politifact Training ROC AUC: 0.98%\n","Politifact Training F1: 0.93%\n","Politifact Training loss: 0.11124854854934699%\n","\n","Gossipcop Training Accuracy: 0.95%\n","Gossipcop Training ROC AUC: 0.98%\n","Gossipcop Training F1: 0.97%\n","Gossipcop Training loss: 0.10521874101139118%\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 394/394 [03:23<00:00,  1.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6:\n","Validation Accuracy: 0.84%\n","Validation ROC AUC: 0.83%\n","Validation F1: 0.90%\n","Validation loss: 0.628752222431228%\n","\n","Politifact Validation Accuracy: 0.71%\n","Politifact Validation ROC AUC: 0.70%\n","Politifact Validation F1: 0.82%\n","Politifact Validation loss: 0.3798285395439182%\n","\n","Gossipcop Validation Accuracy: 0.86%\n","Gossipcop Validation ROC AUC: 0.84%\n","Gossipcop Validation F1: 0.91%\n","Gossipcop Validation loss: 0.6379230949586552%\n","\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 350/1838 [07:21<31:14,  1.26s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-caa253de5fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_from_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-831e51bd9a27>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, model, learning_rate, start_from_epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"QCbgTGQoZUls"},"execution_count":null,"outputs":[]}]}