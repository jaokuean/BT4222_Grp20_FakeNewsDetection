{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clean JSON.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Z82IoUW6YTTl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645977663068,"user_tz":-480,"elapsed":2868,"user":{"displayName":"XingPeng Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18175721296520210782"}},"outputId":"818ac32b-d0f5-4bcc-a4f2-e994bdefaf78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","import re"],"metadata":{"id":"z2j3fgRnMA5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.listdir(\"gdrive/MyDrive/BT4222/Data\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwm4WB6FMq5X","executionInfo":{"status":"ok","timestamp":1645977663070,"user_tz":-480,"elapsed":8,"user":{"displayName":"XingPeng Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18175721296520210782"}},"outputId":"fdcca1c2-4f48-4ed1-c61e-55e934751d9e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['22FEB_fakenewsnet_dataset.zip', 'politifact.json', 'gossipcop.json']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["politifact_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/politifact.json\", \"r\"))\n","gossipcop_data = json.load(open(\"gdrive/MyDrive/BT4222/Data/gossipcop.json\", \"r\"))"],"metadata":{"id":"Yy7c6mJhMD2L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions for processing"],"metadata":{"id":"1VoXZhJgu2x7"}},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rM2CTum4vP_K","executionInfo":{"status":"ok","timestamp":1645974957184,"user_tz":-480,"elapsed":4818,"user":{"displayName":"XingPeng Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18175721296520210782"}},"outputId":"8d2fc22d-b090-4842-9dbc-7b2983510687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting anyascii\n","  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 4.9 MB/s \n","\u001b[?25hCollecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 63.2 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CV00oKL2w713","executionInfo":{"status":"ok","timestamp":1645975361500,"user_tz":-480,"elapsed":157587,"user":{"displayName":"XingPeng Wang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18175721296520210782"}},"outputId":"da08ef5c-beb0-49bd-963f-3d5af10d37be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_lg==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n","\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=b17c38b282873da8a4a3f444603ba03a6d342995418812ec979d25d6e8f780e8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rg853sn8/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"]}]},{"cell_type":"code","source":["import contractions\n","import spacy\n","import string\n","from spacy.tokenizer import Tokenizer\n","from spacy.util import compile_infix_regex"],"metadata":{"id":"bkQ-vnhOv8jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PUNCT = string.punctuation + \"’\""],"metadata":{"id":"0N2Eu35s6wWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_tags(text):\n","    text = re.sub(\"\\n\",' ', text)\n","    text = re.sub(\"\\'\", \"'\", text)\n","    text = re.sub(' +', ' ', text)\n","    return text"],"metadata":{"id":"2S6NP-UbNYMU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def expand_contractions(text):\n","  # creating an empty list\n","  expanded_words = []   \n","  for word in text.split():\n","    # using contractions.fix to expand the shortened words\n","    expanded_words.append(contractions.fix(word))  \n","    \n","  expanded_text = ' '.join(expanded_words)\n","  return expanded_text"],"metadata":{"id":"6GAFN0vcvBlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pipeline():\n","    nlp = spacy.load(\"en_core_web_lg\")\n","    inf = list(nlp.Defaults.infixes)               # Default infixes\n","    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n","    inf = tuple(inf)                               # Convert inf to tuple\n","    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n","    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n","    infix_re = compile_infix_regex(infixes)\n","\n","    custom_tokenizer = Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n","                                suffix_search=nlp.tokenizer.suffix_search,\n","                                infix_finditer=infix_re.finditer,\n","                                token_match=nlp.tokenizer.token_match,\n","                                rules=nlp.Defaults.tokenizer_exceptions)\n","\n","    nlp.tokenizer = custom_tokenizer\n","    return nlp\n","\n","def lemma(text, nlp):\n","    lemma = ' '.join([token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in nlp(text)])\n","    return lemma"],"metadata":{"id":"k0dgzGhswIZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def stopword_removal(text, nlp, stop=None):\n","    all_stopwords = nlp.Defaults.stop_words\n","    if stop:\n","        for word in stop:\n","            all_stopwords.add(word)\n","    doc = nlp(text)\n","    tokens_without_sw= [token.text for token in doc if not token.text in all_stopwords]\n","    filtered_sentence = \" \".join(tokens_without_sw)\n","    return filtered_sentence"],"metadata":{"id":"N2nUgpTF2yzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_punctuation(text, nlp):\n","    doc = nlp(text)\n","    return ' '.join([token.text for token in doc if token.text not in PUNCT])"],"metadata":{"id":"bdsMvrVj5AB1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_pipe(text, nlp):\n","    text = text.lower()\n","    text = remove_tags(text)\n","    text = expand_contractions(text)\n","    text = lemma(text, nlp)\n","    text = stopword_removal(text, nlp)\n","    text = remove_punctuation(text, nlp)\n","    return text"],"metadata":{"id":"UdQcZxM02Jo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = pipeline()"],"metadata":{"id":"KvqEydJmrl4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Clean politifact"],"metadata":{"id":"GVc1iOMj8Ye5"}},{"cell_type":"code","source":["for article in politifact_data:\n","    article['title_clean'] = clean_pipe(article['title'], nlp)\n","    article['text_clean'] = clean_pipe(article['text'], nlp)\n","\n","with open(\"gdrive/MyDrive/BT4222/Data/politifact_clean.json\", 'w') as f:\n","  json.dump(politifact_data,f)"],"metadata":{"id":"z0L5o_EF8HoP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Clean Gossipcop"],"metadata":{"id":"qNV8JEym9t3C"}},{"cell_type":"code","source":["for article in gossipcop_data:\n","    article['title_clean'] = clean_pipe(article['title'], nlp)\n","    article['text_clean'] = clean_pipe(article['text'], nlp)\n","\n","with open(\"gdrive/MyDrive/BT4222/Data/gossipcop_clean.json\", 'w') as f:\n","  json.dump(gossipcop_data,f)"],"metadata":{"id":"OciaHTiH9t3D"},"execution_count":null,"outputs":[]}]}